# Copyright 2024 Intel Corporation.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---

apiVersion: v1
kind: Service
metadata:
  labels:
    app: vllm-llama-app
  name: vllm-llama-svc
  namespace: default
spec:
  ports:
    - port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    app: vllm-llama-app
  type: NodePort

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: vllm-llama-app
  name: vllm-llama
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama-app
  template:
    metadata:
      labels:
        app: vllm-llama-app
    spec:
      containers:
        - image: {{ image }}
          name: vllm-llama-openai
          imagePullPolicy: Always
          workingDir: /root
          env:
            - name: HF_HOME
              value: {{ HF_HOME }}
            - name: http_proxy
              value: {{ http_proxy }}
            - name: https_proxy
              value: {{ https_proxy }}
            - name: no_proxy
              value: localhost,127.0.0.1
            - name: LLM_MODEL
              value: {{ LLM_MODEL }}
            - name: HUGGING_FACE_HUB_TOKEN
              value: "{{ HUGGING_FACE_HUB_TOKEN }}"
            - name: HABANA_VISIBLE_DEVICES
              value: all
            - name: NUM_HPU
              value: "8"
            - name: OMPI_MCA_btl_vader_single_copy_mechanism
              value: none
            - name: PT_HPU_ENABLE_LAZY_COLLECTIVES
              value: "true"
          command:
            - "/bin/sh"
            - "-c"
            - |
              git clone -b v0.5.3.post1-Gaudi-1.17.0 https://github.com/HabanaAI/vllm-fork.git
              cd vllm-fork
              pip install -e .
              pip install setuptools==69.0.0
              python -m vllm.entrypoints.openai.api_server --model $LLM_MODEL --dtype auto  --block-size 128 --max-num-seqs 128 --gpu-memory-utilization 0.8 --tensor-parallel-size $NUM_HPU
              sleep infinity
          ports:
            - containerPort: 8000
              protocol: TCP
          resources:
            limits:
              habana.ai/gaudi: 8
              memory: 400Gi
              hugepages-2Mi: 95000Mi
            requests:
              habana.ai/gaudi: 8
              memory: 400Gi
              hugepages-2Mi: 95000Mi
          volumeMounts:
            - name: datasets
              mountPath: {{ mountPath }}
      volumes:
        - name: datasets
          persistentVolumeClaim:
            claimName: {{ pvc_name }}
            readOnly: false

---
apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: gaudi-llm-ds-ft
  namespace: workloads
spec:
  slotsPerWorker: 8
  runPolicy:
    cleanPodPolicy: Running
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          containers:
            - image: vault.habana.ai/gaudi-docker/1.19.1/ubuntu22.04/habanalabs/pytorch-installer-2.5.1:latest
              # In case of Ubuntu 24.04 OS, use image:vault.habana.ai/gaudi-docker/1.18.0/ubuntu24.04/habanalabs/pytorch-installer-2.4.0:latest
              name: gaudi-llm-ds-ft-launcher
              env:
                - name: HF_HOME
                  value: /storage/huggingface
                - name: http_proxy
                  value: ""
                - name: https_proxy
                  value: ""
                - name: no_proxy
                  value: "127.0.0.1,localhost"
                - name: LLM_MODEL
                  value: "meta-llama/Meta-Llama-3.1-8B-Instruct"
                - name: NUM_HPU
                  value: "8"
                - name: NUM_EPOCHS
                  value: "3"
                - name: HUGGING_FACE_HUB_TOKEN
                  value: ""
              command: ["/bin/bash", "-c"]
              args:
                - >-
                  HOSTSFILE=$OMPI_MCA_orte_default_hostfile;
                  MASTER_ADDR="$(head -n 1 $HOSTSFILE | sed -n s/[[:space:]]slots.*//p)";
                  SETUP_CMD="git clone  https://github.com/huggingface/optimum-habana /optimum-habana";
                  export no_proxy=$no_proxy,$KUBERNETES_SERVICE_HOST;
                  NUM_NODES=$(wc -l < $HOSTSFILE);
                  N_CARDS=$((NUM_NODES*NUM_HPU));

                  git clone  https://github.com/huggingface/optimum-habana /optimum-habana;
                  cd /optimum-habana;
                  git checkout v1.14.1;
                  sed -i '194s|deepspeed|deepspeed --force_multi|' optimum/habana/distributed/distributed_runner.py;
                  pip install .;
                  pip install -r examples/language-modeling/requirements.txt;
                  pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.19.0;

                  mpirun --npernode 1 \
                    --tag-output \
                    --allow-run-as-root \
                    --prefix $MPI_ROOT \
                    -x http_proxy=$http_proxy \
                    -x https_proxy=$https_proxy \
                    -x no_proxy=$no_proxy \
                    -x LLM_MODEL=$LLM_MODEL \
                    -x HF_HOME=$HF_HOME \
                    -x NUM_HPU=$NUM_HPU \
                    -x HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \
                    bash -i -c '
                    git clone  https://github.com/huggingface/optimum-habana /optimum-habana
                    cd /optimum-habana
                    git checkout v1.14.1
                    hf_home_var="os.environ[\"HF_HOME\"] = \"${HF_HOME}\""
                    token_var="os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"${HUGGING_FACE_HUB_TOKEN}\""
                    https_var="os.environ[\"https_proxy\"] = \"${https_proxy}\""
                    http_var="os.environ[\"http_proxy\"] = \"${http_proxy}\""
                    no_proxy_var="os.environ[\"no_proxy\"] = \"${no_proxy}\""
                    sed -i "56i\\${https_var}" examples/language-modeling/run_lora_clm.py
                    sed -i "57i\\${http_var}" examples/language-modeling/run_lora_clm.py
                    sed -i "58i\\${hf_home_var}" examples/language-modeling/run_lora_clm.py
                    sed -i "59i\\${token_var}" examples/language-modeling/run_lora_clm.py
                    sed -i "60i\\${no_proxy_var}" examples/language-modeling/run_lora_clm.py

                    pip install .
                    pip install -r examples/language-modeling/requirements.txt
                    pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.19.0
                    ';

                    cd /optimum-habana/examples/language-modeling/;
                    python ../gaudi_spawn.py --hostfile=$HOSTSFILE --use_deepspeed --world_size $N_CARDS  run_lora_clm.py --model_name_or_path $LLM_MODEL \
                    --dataset_name tatsu-lab/alpaca \
                    --bf16 True \
                    --output_dir ./model_lora_llama_ddp \
                    --num_train_epochs $NUM_EPOCHS \
                    --per_device_train_batch_size 8 \
                    --gradient_accumulation_steps 2 \
                    --evaluation_strategy "no" \
                    --save_strategy "no" \
                    --learning_rate 3e-4 \
                    --warmup_ratio  0.03 \
                    --lr_scheduler_type "constant" \
                    --max_grad_norm  0.3 \
                    --logging_steps 1 \
                    --do_train \
                    --do_eval \
                    --use_habana \
                    --use_lazy_mode \
                    --throughput_warmup_steps 3 \
                    --lora_rank=8 \
                    --lora_alpha=16 \
                    --lora_dropout=0.05 \
                    --lora_target_modules "q_proj" "v_proj" \
                    --dataset_concatenation \
                    --max_seq_length 512 \
                    --ddp_bucket_cap_mb 50 \
                    --adam_epsilon 1e-08 \
                    --validation_split_percentage 4 \
                    --low_cpu_mem_usage True;

              volumeMounts:
                - name: datasets
                  mountPath: /storage
          volumes:
            - name: datasets
              persistentVolumeClaim:
                claimName: shared-model
                readOnly: false
    Worker:
      replicas: 1
      template:
        spec:
          hostIPC: true
          containers:
            - image: vault.habana.ai/gaudi-docker/1.19.1/ubuntu22.04/habanalabs/pytorch-installer-2.5.1:latest
              # In case of Ubuntu 24.04 OS, use image:vault.habana.ai/gaudi-docker/1.18.0/ubuntu24.04/habanalabs/pytorch-installer-2.4.0:latest
              name: gaudi-llm-ds-ft-worker
              resources:
                limits:
                  habana.ai/gaudi: 8
                  memory: 250Gi
                  hugepages-2Mi: 312600Mi
                requests:
                  habana.ai/gaudi: 8
                  memory: 250Gi
                  hugepages-2Mi: 312600Mi
              volumeMounts:
                - name: datasets
                  mountPath: /storage
          volumes:
            - name: datasets
              persistentVolumeClaim:
                claimName: shared-model
                readOnly: false

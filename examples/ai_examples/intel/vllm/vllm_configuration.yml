---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: vllm-llama-app
  name: vllm-llama-svc
  namespace: workloads
spec:
  ports:
    - port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    app: vllm-llama-app
  type: NodePort

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: vllm-llama-app
  name: vllm-llama
  namespace: workloads
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama-app
  template:
    metadata:
      labels:
        app: vllm-llama-app
    spec:
      containers:
        - image: vault.habana.ai/gaudi-docker/1.19.1/ubuntu22.04/habanalabs/pytorch-installer-2.5.1:latest
          name: vllm-llama-openai
          imagePullPolicy: Always
          workingDir: /root
          env:
            - name: HF_HOME
              value: /storage/huggingface
            - name: http_proxy
              value: ""
            - name: https_proxy
              value: ""
            - name: no_proxy
              value: "127.0.0.1,localhost"
            - name: LLM_MODEL
              value: "meta-llama/Meta-Llama-3.1-8B-Instruct"
            - name: HUGGING_FACE_HUB_TOKEN
              value: ""
            - name: HABANA_VISIBLE_DEVICES
              value: all
            - name: NUM_HPU
              value: "8"
            - name: OMPI_MCA_btl_vader_single_copy_mechanism
              value: none
            - name: PT_HPU_ENABLE_LAZY_COLLECTIVES
              value: "true"
          command:
            - "/bin/sh"
            - "-c"
            - |
              git clone -b v0.6.4.post2+Gaudi-1.19.0 https://github.com/HabanaAI/vllm-fork.git
              cd vllm-fork
              pip install -v -r requirements-hpu.txt
              export VLLM_TARGET_DEVICE=hpu
              python3 setup.py install
              python3 -m vllm.entrypoints.openai.api_server --model $LLM_MODEL --dtype auto  --block-size 128 --max-num-seqs 128 --gpu-memory-utilization 0.5 --tensor-parallel-size $NUM_HPU
          ports:
            - containerPort: 8000
              protocol: TCP
          resources:
            limits:
              habana.ai/gaudi: 8
              memory: 400Gi
              hugepages-2Mi: 312600Mi
            requests:
              habana.ai/gaudi: 8
              memory: 400Gi
              hugepages-2Mi: 312600Mi
          volumeMounts:
            - name: datasets
              mountPath: /storage
      volumes:
        - name: datasets
          persistentVolumeClaim:
            claimName: shared-model
            readOnly: false

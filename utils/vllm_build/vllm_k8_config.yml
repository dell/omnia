apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-3-70b
spec:
  predictor:
    containers:
    - args:
        - "--port"
        - "8080"
        - "--model"
        - "meta-llama/Meta-Llama-3-70B-Instruct"
      command:
        - "python3"
        - "-m"
        - "vllm.entrypoints.api_server"
      env:
      - name: HUGGING_FACE_HUB_TOKEN
        value: "xxxxxxxxxxxxxxxxxxxxx"
      - name: HTTP_PROXY
        value: "http://10.20.0.1:3128"
      - name: HTTPS_PROXY
        value: "http://10.20.0.1:3128"
      image: vllm-rocm:latest
      imagePullPolicy: IfNotPresent
      name: vllm-container
      resources:
        limits:
          cpu: "4"
          memory: 600Gi
          amd.com/gpu: "4"
        requests:
          cpu: "1"
          memory: 200Gi
          amd.com/gpu: "4"
          

#  Copyright 2022 Dell Inc. or its subsidiaries. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
---

# Usage: main.yml
awx_search_key: "-job-"
base_vars_path: "{{ playbook_dir }}/control_plane/input_params/base_vars.yml"
cobbler_pod_name: cobbler
omnia_log_path: /var/log/omnia
cp_path: "/root/.ansible/cp/"
ansible_cfg_src: "{{ playbook_dir }}/ansible.cfg"
ansible_cfg_dest: 
  - { path: "{{ playbook_dir }}/control_plane/tools/ansible.cfg", log_path: "/var/log/omnia/omnia_control_plane_tools.log", regexp: "/var/log/omnia.log" }
  - { path: "{{ playbook_dir }}/telemetry/ansible.cfg", log_path: "/var/log/omnia/omnia_telemetry.log", regexp: "/var/log/omnia.log" }
  - { path: "{{ playbook_dir }}/tools/ansible.cfg", log_path: "/var/log/omnia/omnia_tools.log", regexp: "/var/log/omnia.log" }
  - { path: "{{ playbook_dir }}/platforms/ansible.cfg", log_path: "/var/log/omnia/omnia_platforms.log", regexp: "/var/log/omnia.log" }

# Usage: fetch_omnia_inputs.yml
config_filename: "omnia_config.yml"
config_vaultname: .omnia_vault_key
min_length: 8
max_length: 30
fail_msg_mariadb_password: "maria_db password not given in correct format."
success_msg_mariadb_password: "mariadb_password validated"
success_msg_k8s_version: "Kubernetes Version Validated"
fail_msg_k8s_version: "Failed. Kubernetes Version is unsupported or incorrect in omnia_config.yml"
success_msg_k8s_cni: "Kubernetes CNI Validated"
fail_msg_k8s_cni: "Kubernetes CNI not correct."
success_msg_k8s_pod_network_cidr: "Kubernetes pod network cidr validated"
fail_msg_k8s_pod_network_cidr: "Kubernetes pod network cidr not given in correct format"
file_perm: '0755'
domain_name_length: '63'
domain_name_success_msg: "domain name successfully validated"
domain_name_fail_msg: "Failed. Incorrect format provided for domain name in omnia_config.yml"
realm_name_success_msg: "realm_name successfully validated"
realm_name_fail_msg: "Failed. Incorrect realm_name formate in omnia_config.yml"
success_msg_directory_manager_password: "directory_manager_password successfully validated"
fail_msg_directory_manager_password: "Failed. Incorrect format provided for directory_manager_password"
success_msg_kerberos_admin_password: "kerberos_admin_password successfully validated"
fail_msg_kerberos_admin_password: "Failed. Incorrect format provided for kerberos_admin_password"
input_config_failure_msg: "Input parameters cannot be empty"
login_node_required_success_msg: "login_node_required successfully validated"
login_node_required_fail_msg: "Failed. login_node_required should be either true or false"
secure_login_node_success_msg: "enable_secure_login_node successfully validated"
secure_login_node_fail_msg: "Failed. enable_secure_login_node should be either true or false"
ipa_secret_file: "{{ playbook_dir }}/control_plane/roles/control_plane_security/files/.ipavars.yml"
beegfs_support_success_msg: "beegfs_support validated successfully"
beegfs_support_failure_msg: "Failed. beegfs_support should be either true or false"

# Usage: fetch_beegfs_inputs.yml
beegfs_rdma_support_success_msg: "beegfs_rdma_support validated successfully"
beegfs_rdma_support_failure_msg: "Failed. beegfs_rdma_support should be either true or false"
beegfs_mgmt_server_success_msg: "beegfs_mgmt_server, validated successfully"
beegfs_mgmt_server_fail_msg: "Failed. Please enter IP of management server in IPV4 format"
beegfs_mgmt_server_not_reachable_msg: "Failure, IP provided for beegfs_mgmt_server is not reachable. Please provide reachable IP of beegfs management server"
ping_msg: "100% packet loss"
beegfs_mounts_fail_msg: "Failed. Please provide a path for mounting beegfs file system"
beegfs_client_version_success_msg: "beegfs_client_version, validated successfully "
beegfs_client_version_fail_msg: "Failed, Please provide beegfs_client_version grater than 7.2"
ofed_kernel_fail_msg: "Failed. beegfs_rdma_support must be true for configuring separate drivers"
beegfs_non_leap_repo: "https://www.beegfs.io/release/beegfs_{{ hostvars['127.0.0.1']['beegfs_client_version'] }}/dists/beegfs-rhel8.repo"
beegfs_repo_status_msg: "Failed. Value entered for beegfs_client_version is not valid beegfs client version. Please enter valid beegfs client version by checking beegfs release page"
beegfs_unmount_client_success_msg: "beegfs_unmount_client validated successfully"
beegfs_unmount_client_failure_msg: "beegfs_unmount_client value should be either true or false"
beegfs_version_change_success_msg: "beegfs_version_change validated successfully"
beegfs_version_change_failure_msg: "beegfs_version_change value should be either true or false"
unmount_success_msg: "beegfs_unmount_client validated successfully"
unmount_failure_msg: "beegfs_unmount_client should be true only when beegfs_version change is true or there is a change in mount location"
latest_version: 7.3.0
compatability_msg: "Upgradation to 7.3.0 is not possible, as it is not supported by beegfs, try upgrading to some other versions"

# Usage: validations.yml
skip_tag_fail_msg: "Can't skip both slurm and kubernetes"
manager_group_fail_msg: "manager group should contain exactly 1 node"
manager_group_success_msg: "manager group check passed"
compute_group_fail_msg: "compute group should contain atleast 1 node"
compute_group_success_msg: "compute group check passed"
disjoint_fail_msg: "manager and compute groups should be disjoint"
disjoint_success_msg: "manager and compute groups are disjoint"
login_node_group_fail_msg: "Login node group should contain atleast 1 node when login_node_required is true"
login_node_group_success_msg: "Login node group check passed when login_node_required is true"
nfs_node_group_fail_msg: "nfs_node group should contain exactly 1 node"
nfs_node_group_success_msg: "nfs_node group check passed"

# Usage: fetch_powervault_status.yml
pv_ip_undefined_msg: "Please give IP of powervault connected to nfs_node in omnia_config.yml in correct format"
powervault_login_failure_msg: "Unable to connect to powervault. Please check powervault_ip in omnia_config.yml and powervault credentials in login_vars.yml"

# Usage: fetch_control_plane_credentials.yml
login_vars_filename: input_params/login_vars.yml
vault_filename: input_params/.login_vault_key
vault_file_perm: '0644'

# Usage: fetch_security_inputs.yml
security_vars_filename: "{{ playbook_dir }}/omnia_security_config.yml"
max_failures_success_msg: "max_failures successfully validated"
max_failures_fail_msg: "Failed. Incorrect max_failures value in security_vars.yml"
failure_reset_interval_success_msg: "failure_reset_interval successfully validated"
failure_reset_interval_fail_msg: "Failed. Incorrect failure_reset_interval value in security_vars.yml"
lockout_duration_success_msg: "lockout_duration successfully validated"
lockout_duration_fail_msg: "Failed. Incorrect lockout_duration value in security_vars.yml"
session_timeout_success_msg: "session_timeout successfully validated"
session_timeout_fail_msg: "Failed. Incorrect session_timeout value in security_vars.yml"
max_failures_default_value: 3
failure_reset_interval_min_value: 30
failure_reset_interval_max_value: 60
lockout_duration_min_value: 5
lockout_duration_max_value: 10
session_timeout_min_value: 90
session_timeout_max_value: 180
alert_email_success_msg: "alert_email_address successfully validated"
alert_email_fail_msg: "Failed. Incorrect alert_email_address value in security_vars.yml"
alert_email_warning_msg: "[WARNING] alert_email_address is empty. Authentication failure alerts won't be configured."
email_max_length: 320
email_search_key: "@"
user_success_msg: "user successfully validated"
user_fail_msg: "Failed. Incorrect user format in security_vars.yml"
allow_deny_success_msg: "Access successfully validated"
allow_deny_fail_msg: "Failed. Incorrect Access format in security_vars.yml"
restrict_program_support_success_msg: "restrict_program_support successfully validated"
restrict_program_support_failure_msg: "Failed. Accepted values are true or false."
restrict_softwares_success_msg: "restrict_softwares successfully validated"
restrict_softwares_failure_msg: "Warning. Values should be comma separated. The supported services are telnet, lpd, bluetooth, rlogin, rexec. Please check restrict_softwares variable"

# Usage: install_packages.yml
ansible_base_version: '2.9'
ipaddr_collection: ansible.utils:2.5.2
ansible_collection_nfs: ansible.posix:1.4.0

# Usage: fetch_nfs_client_params.yml
nfs_server_reach_msg: "Failed. Nfs server is not reachable. Please enter valid nfs server IP in nfs_client_params"
server_share_path_success_msg: "server_share_path is validated successfully"
server_share_path_failure_msg: "Failed. Please enter valid NFS server mount location"
server_ip_success_msg: "Successfully verified server_ip variable declaration"
server_ip_fail_msg: "Failed, Declare server_ip variable in nfs_client_params of omnia_config, please refer comments in omnia_config.yml"
server_share_path_fail_msg: "Failed, Declare server_share_path variable in nfs_client_params of omnia_config, please refer comments in omnia_config.yml"
client_share_path_success_msg: "Successfully verified client_share_path variable declaration"
client_share_path_fail_msg: "Failed, Declare client_share_path variable in nfs_client_params of omnia_config, please refer comments in omnia_config.yml"
server_share_path_len_success_msg: "server_share_path validated successfully"
client_mount_options_success_msg: "client_mount_options declaration validated successfully"
client_mount_options_fail_msg: "Failed, please declare client_mount_options variable in nfs_client_params"
server_ip_msg: "As server_ip is not available NFS bolt-on role will not work"
nfs_client_declaration_msg: "Make sure to declare values in server_ip and server_share_path in all the lists on nfs_client_params"
